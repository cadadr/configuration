# antenna2json.py --- extract favourites, subscriptions, and queue from multiple antennapod database exports

# Copyright (C) 2023 Göktuğ Kayaalp <self at gkayaalp dot com>

"""Extract favourites, subscriptions, and queue from multiple AntennaPod database exports.

This script will combine the information about the above data from
multiple (arbitrarily many) AntennaPod database exports and export
them as a singular JSON structure, with duplicates cleaned.

The output is a UTF-8 encoded JSON document that consists of an object
with three arrays in it: \"favourites\", \"subscriptions\", and
\"queue\".

Favourites and queue data share their structure, they are arrays made
up of objects that have the following keys:

- \"feed_item_link\":      The link for the podcast episode
- \"feed_item_media_url\": The link to the episode enclosure (the audio file)
- \"feed_item_title\":     The title of the episode
- \"feed_link\":           The link for the podcast itself (usually the podcast homepage)
- \"feed_title\":          Podcast's title
- \"feed_url\":            The URL of podcast's RSS/Atom/JSON feed

\"favourites\" are the favourited episodes, and \"queue\" holds
enqueued episodes.

Subscriptions, i.e. the podcasts that you follow on Antennapod, has a
different set of keys:

- \"download_url\": The URL of podcast's RSS/Atom/JSON feed
- \"link\":         The link for the podcast itself (usually the podcast homepage)
- "title":          Podcast's title

There are some other fields of the toplevel object that store some
metadata:

- \"comment\": Mentions that the JSON was generated by this script,
  and gives human readable local time for when the output was
  generated, approximately

- \"generated_on_iso8601date\": Machine readable ISO 8601 UTC date and
  time, must be the same as the date-time above otherwise

- \"favs_sources\": File names of the databases (AntennaPod exports)
  from which favourites were collected

- \"queue_and_subs_source\": Database (AntennaPod export) from which
  queued episodes and subscribed feeds were collected from

The script outputs to standard output, use shell redirection to save
the generated JSON.

"""

import collections
import datetime
import json
import pathlib
import sqlite3
import sys

query_favs = """
    select
        FeedItems.title as feed_item_title,
        FeedItems.link as feed_item_link,
        Feeds.title as feed_title,
        Feeds.link as feed_link,
        Feeds.download_url as feed_url,
        FeedMedia.download_url as feed_item_media_url
    from
    FeedItems inner join Favorites
              on FeedItems.id = Favorites.feeditem
              left join Feeds
              on Feeds.id = FeedItems.feed
              left join FeedMedia
              on FeedItems.id = FeedMedia.feeditem
    ;
    """

query_subs = "select title,download_url,link from Feeds;"

query_queue = """
    select
        FeedItems.title as feed_item_title,
        FeedItems.link as feed_item_link,
        Feeds.title as feed_title,
        Feeds.link as feed_link,
        Feeds.download_url as feed_url,
        FeedMedia.download_url as feed_item_media_url
    from
    FeedItems inner join Queue
              on FeedItems.id = Queue.feeditem
              left join Feeds
              on Feeds.id = FeedItems.feed
              left join FeedMedia
              on FeedItems.id = FeedMedia.feeditem
    ;
    """

databases = set()

# Make sure databases exist cos sqlite is a dick and it’ll just make
# the databases. Good luck if you accidentally misquote a glob...
for db in sys.argv[1:]:
    path = pathlib.Path(db)
    if path.is_file():
        databases.add(path)
    else:
        sys.stderr.write(f"file does not exist: `{db}'\n")
        exit(1)

if databases == []:
    sys.stderr.write(f"usage: {sys.argv[0]} DATABASE [DATABASES...] [> OUTPUT_FILE]\n")
    exit(1)

databases = sorted(databases)

favourites = []
favs_cols = ("feed_item_title", "feed_item_link", "feed_title",
             "feed_link", "feed_url", "feed_item_media_url")
favs_set   = set()

for db in databases:
    con = sqlite3.connect(db)
    cur = con.cursor()

    try:
        results = cur.execute(query_favs)
    except sqlite3.OperationalError as e:
        sys.stderr.write(f"sqlite3 error: {e}\ndatabase: {db}\n")
        exit(1)
    for fav in results:
        if fav not in favs_set:
            favs_set.add(fav)
            favourites.append(dict(zip(favs_cols, fav)))
    else:
        cur.close()
else:
    con.close()


# Get queue and subs. We don’t wanna merge across databases here but
# instead just pick one, probably the most recent one. So, prompt the
# user about it.

sys.stderr.write("Pick which database to export Queue and Subscriptions from.\n")
for i, db in enumerate(databases):
    sys.stderr.write(f"{i}:\t{db}\n")

answer = None

while answer not in range(0, i + 1):
    if answer is not None:
        sys.stderr.write(f"Please pick a number between 0 and {i}\n")
    try:
        sys.stderr.write(f"[0-{i}]: ")
        answer = int(input())
    except ValueError as e:
        sys.stderr.write(f"{e}\n")

sys.stderr.write(f"using {databases[answer]}...\n")

con = sqlite3.connect(databases[answer])

subscriptions = []
subs_cols = ("title", "download_url", "link")

queue = []
queue_cols = favs_cols

cur = con.cursor()

for sub in cur.execute(query_subs):
    subscriptions.append(dict(zip(subs_cols, sub)))

for queued in cur.execute(query_queue):
    queue.append(dict(zip(queue_cols, queued)))

cur.close()

con.close()

now = datetime.datetime.now()
tz  = now.astimezone().tzinfo

data = collections.OrderedDict([
    # metadata
    ("comment", f"Generated by antenna2json.py, on {now.ctime()} (local time at {tz})"),
    # .replace(...) ensures the (non-)offset for UTC is included in the
    # string returned by .isoformat().
    ("generated_on_iso8601date", f"{now.replace(tzinfo=datetime.timezone.utc).isoformat()}"),
    ("favs_sources", [str(d.absolute()) for d in databases]),
    ("queue_and_subs_source", str(databases[answer].absolute())),
    # data
    ("favourites",    favourites),
    ("subscriptions", subscriptions),
    ("queue",         queue)
])

# Dump data as json
try:
    print(json.dumps(data, sort_keys=False, indent=4, ensure_ascii=False))
except BrokenPipeError:         # pipe closed, like |head
    pass

sys.stderr.write(
    f"Done. Collected {len(favourites)} favs, "
    f"{len(subscriptions)} subs, and {len(queue)} queued episode(s).\n"
)
